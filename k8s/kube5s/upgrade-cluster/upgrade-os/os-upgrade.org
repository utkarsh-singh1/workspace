#+title: os-upgrade
#+author: UTKARSH SINGH


* Pod-eviction

In a 3 worker node (w1, w2, w3) and 1 master node (m1) ,carrying these color labelled pods, each node carries 2 pods like this -

- w1 -> blue, green,
- w2 -> blue, red
- w3 -> red, black

Now, if w1 goes down blue and green pods goes down also, however app running in pod does not seem to be affected as it has one more replica in w2, but green pod does not, its a issue, however, if node w1 comes before 5 min, it can revive blue and green pods and service can resume however, if it does not pods will be deleted if node goes down for more than 5 min. This 5 min timelimit is set in kube-controller-manager and also known as pod-eviction time-out. In case if pod that is deleted was part of replicaset it can be scheduled on other worker nodes depends upon node availability, but if they are not part of it so say bye-bye to your app.

So if somehow you want to maintain a node and want to take down node w1 and repair it U can use -

#+begin_src sh
  kubectl drain w1
#+end_src

This will drain node w1 and schedule its previous pods (blue, greeb) in other nodes based on avaialability.

However, if node w1 still comes back online previous pods can not go back to w1 due to -

- Its cordoned or tainted unschedulable
- Its not magic you delete these previous pods (blue and green) from other nodes than schedule on w1.

However, before scheduling any new pods on w1 you have to remove its taint by -

#+begin_src bash
  kubectl uncordon w1
#+end_src

And new pods can be scheduled on this w1.

You can use-

#+begin_src bash
  kubectl cordon w1
#+end_src

Make w1 cordon or unschedulable and no new pods can be placed on it. However unlike drain it does not move existing pods to other nodes it just make node unschedulable for new pods.

* K8s-Releases and Versioning

kubernetes versioning is what current api-versioning is -

You can see that by running -

#+begin_src bash 
  kubectl get nodes -o wide
#+end_src

Output -
Version section is called as current k8s version.

#+begin_quote
NAME      STATUS     ROLES           AGE    VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE              KERNEL-VERSION   CONTAINER-RUNTIME
k8s       Ready      control-plane   104s   v1.32.0   192.168.39.141   <none>        Buildroot 2023.02.9   5.10.207         docker://27.4.0
k8s-m02   NotReady   <none>          37s    v1.32.0   192.168.39.60    <none>        Buildroot 2023.02.9   5.10.207         docker://27.4.0
k8s-m03   NotReady   <none>          10s    v1.32.0   192.168.39.240   <none>        Buildroot 2023.02.9   5.10.207         docker://27.4.0
#+end_quote

Which is currently v1.32.0

Ok so what is v1.32.0 describes -

- 1 - Major Version
- 32 - Minor Version (Introduces festures, performance upgrades and functinalitites, Introduces in every few months cycle)
- 0 or >0 - Patch Version (Including Bug fixes)

There are alpha and beta releases of k8s are also there -

- Intial imporovements and bug fixes are transported to the alpha releases where features are disabled intially.
- Then they are gone to beta releases where are codes are well tested and enabled in the release and finally it is merged into the stable release.

* Cluster-Upgrade Process

Kubernetes supports upto 3 versions of k8s release means -> x , x-1 , x-2 with x being the current version.

Also, kubernetes master plane components follow a hierarchy order in support to the components -

- Kube-apiserver is major component, where all major component talk to the kube-apiserver so it must be at the version x (x being current k8s version Ex. 1.32)
- Then comes controller-manager and scheduler that can be at x or x-1 to the apiserver.
- then kube-proxy or kubelet which can be at x/x-1/x-2 to the apiserver.
- But kubectl can be at x/x-1/x+1 to the kube-apiserver.

Remember as previously stated you can have support of k8s upto max x-2 version of x (x being current version), if x becomes x+1 then x-2 will be unsupported, so what will happen you have to upgrade to the newest cluster, but not direct from x-2 to x+1 or x but by being periodically one by one, x-2>x-1>x>x+1.

There are several tools that can be used to upgrade cluster like, cloud-service-provider, kubeadm or hard-way (manually dowloading components and upgrading it.)

Here we will do it by kubeadm way.

- Intially there are mulitple methodologies to upgrade kubernetes cluseter
  - Starting from master node m1 (Always start from master node) while we upgrade m1 by taking it down (meanining all master plane componentsa are not available but worker nodes w1,w2,w3 are available means sevices fro blue green orange and black pods are active. But creating new pods or resurces and deleting them etc are not allowed). After master node comes back up, all services will resume also other components as compared to kube-apiserver are still behind by one or two version it looks good as kubelet and kube-proxy can be x-2/x-1/x from apiserver.
  - But as usual u can upgrade w1 w2 w3/m1 in multiple different ways as stated earlier.
    - Drain w1/w2/w3 all at once to upgrade it then restore service. But since all nodes are down users can not be accessing the apps as all the nodes are down.
    - Or Upgrade one at a time. First of w1/w2/w3 in any order.  
    - Or introduced new upgraded nodes in the cluster containing apps and services while replacing old nodes. This can be possible in cloud env.

  Anyway one way to upgrade a node in kubeadm is to use

  #+begin_src sh
    kubeadm upgrade plan
  #+end_src

  and

  #+begin_src sh
    kubeadm upgrade apply
  #+end_src


** Steps to upgrade cluster version -

*** Steps to upgrade master node 

  1. Run ~kubectl upgrade plan~ which will give you all the information about cluster and kubelet and which latest version you can upgrade your kubeadm and cluster onto.
     But you have to manually upgrade the kubelet as kubeadm does not install or upgrade kubelet and restart kubelet. But you must upgrade kubeadm tool itself before upgrading the cluster.

     Remember we can only upgrade cluster version one at a time. Ex. 1.x -> 1.(x+1), 1.2 -> 1.21 etc.
     
     So run - 

     #+begin_src sh
       #if pkg manager is apt
       apt-get upgrade -y kubeadm=version-to-upgrade
     #+end_src

 2. Then run -

     #+begin_src sh
       kubeadm upgrade apply version-to-upgrade
     #+end_src

     This command will upgrade the master-node components, but when you will run

     #+begin_src shell
       kubectl get nodes
     #+end_src

     You will still get previous version in the output because kubeadm has not upgraded the kubelet yet, reasone as usual kubeadm does not install or upgrade kubelet, so version show here belongs to kubelet not to the kube-apiserver. 

 3. Means kubelet pkg needs to be upgraded and restart its services -

    #+begin_src sh
      # if you are on apt pkg manager
      apt-get upgrade -y kubelet=version-to-upgrade
    #+end_src

    #+begin_src sh
      # In case of systemD
      systemctl restart kubelet.service
    #+end_src

    Now master node Upgrade is complete.

*** Steps to Upgrade worker nodes

Switch to the node you want to upgrade.

1. We have to do it one bye one (In case of we have more than 1 worker node (w1,w2,w3) ). First run -

   #+begin_src sh
     kubectl drain w1
   #+end_src

   which will transfer loads of node1 to other nodes. And make it unschedulable.

   Now node1 is down to upgrade.

2. Like master node worker node needs to upgrade kubeadm and kubelet to desired state and restart the kubelet to let node1 upgrade.

   #+begin_src shell
     # If pkg Manager is apt
     apt-get upgrade -y kubeadm=version-to-upgrade
     apt-get upgrade -y kubelet=version-to-upgrade
   #+end_src

   Now update the new node for new kubelet config

   #+begin_src shell
     kubeadm upgrade node config --kubelet-version latest-upgraded-version-of-kubelet
   #+end_src

   Now restart the service

   #+begin_src sh
     systemctl restart kubelet.service
   #+end_src
   
3. Now it is upgraded but remember it needed to be uncordoned or remove the unschedulable taint from w1 

   #+begin_src shell
     kubectl uncordon w1
   #+end_src

   Remember old pods will not come back automatically, they will be back when they are deleted or rescheduled to the w1.
   
   Similiarly with this method we can upgrade all the Nodes to the latest version.
